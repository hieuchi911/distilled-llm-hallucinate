from transformers import pipeline, AutoTokenizer, AutoConfig, AutoModelForCausalLM, MT5ForConditionalGeneration
import torch

class LanguageModel(torch.nn.Module):
    def __init__(self, config, language_model) -> None:
        super(LanguageModel, self).__init__()
        if config.is_encoder_decoder:
            self.model = MT5ForConditionalGeneration.from_pretrained(language_model)
        else:
            self.model = AutoModelForCausalLM.from_pretrained(language_model)
    
    def forward(self, input_ids, attention_mask, max_new_tokens, use_cache, pad_token_id):
        # to ensure outputs generated by models across gpus have the same
        # length so nn.parallel.gather will not throws error, set `min_new_tokens`
        # and `max_new_tokens` both to the same value
        outputs = self.model.generate(input_ids=input_ids, attention_mask=attention_mask,
                                max_new_tokens=max_new_tokens, use_cache=use_cache,
                                min_new_tokens=max_new_tokens,
                                pad_token_id=pad_token_id)
        return outputs

def prepare_model(model_name, tokenizer_name):
    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name, padding_side='left')
    tokenizer.pad_token = tokenizer.eos_token

    config = AutoConfig.from_pretrained(model_name)
    model = LanguageModel(config, model_name)

    if torch.cuda.device_count() > 1:
        model = torch.nn.DataParallel(model)

    return model, tokenizer, config

def prepare_ddp_model(model_name, tokenizer_name):
    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name, padding_side='left')
    tokenizer.pad_token = tokenizer.eos_token

    config = AutoConfig.from_pretrained(model_name)
    if config.is_encoder_decoder:
        model = MT5ForConditionalGeneration.from_pretrained(model_name)
    else:
        model = AutoModelForCausalLM.from_pretrained(model_name)
    
    return model, tokenizer, config

