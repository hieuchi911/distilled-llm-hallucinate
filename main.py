from tqdm.auto import tqdm
from typing import Sequence, NamedTuple
import logging
import json
import argparse

import evaluate
import datasets
from transformers import pipeline, AutoTokenizer, AutoConfig, AutoModelForCausalLM, MT5ForConditionalGeneration
import torch
from transformers.pipelines.pt_utils import KeyDataset

from data_utils import load_module_from_py_file, calculate_avg_tokens, collate_fn,\
            get_max_length, evaluate_lm, DATA_PATHS, PROMPT_KEYWORDS

logging.basicConfig(filename='evalscript',
                    filemode='w',
                    format='%(asctime)s,%(msecs)d %(name)s %(levelname)s %(message)s',
                    datefmt='%H:%M:%S',
                    level=logging.DEBUG)

logging.info("Running EvalScript")

logger = logging.getLogger('evalscript')

class LLMTokenizerMapping(NamedTuple):
    """Store data about tokenizers used for an LLM"""
    language_model: str
    tokenizer: str

class LanguageModel(torch.nn.Module):
    def __init__(self, config, language_model) -> None:
        super(LanguageModel, self).__init__()
        if config.is_encoder_decoder:
            self.model = MT5ForConditionalGeneration.from_pretrained(language_model)
        else:
            self.model = AutoModelForCausalLM.from_pretrained(language_model)
    
    def forward(self, input_ids, attention_mask, max_new_tokens, use_cache, pad_token_id):
        # to ensure outputs generated by models across gpus have the same
        # length so nn.parallel.gather will not throws error, set `min_new_tokens`
        # and `max_new_tokens` both to the same value
        outputs = self.model.generate(input_ids=input_ids, attention_mask=attention_mask,
                                max_new_tokens=max_new_tokens, use_cache=use_cache,
                                min_new_tokens=max_new_tokens,
                                pad_token_id=pad_token_id)
        return outputs


def main(args):
    device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

    bertscore = evaluate.load("bertscore")
    rougescore = evaluate.load('rouge')
    chrfscore = evaluate.load("chrf")
    meteorscore = evaluate.load('meteor')    

    dataset = datasets.load_dataset(DATA_PATHS[args.dataset], args.subset, split="validation")
    # Apply the function to each sample using map
    map_fn = getattr(load_module_from_py_file(args.data_mapper), f"add_prefix_{args.dataset}")
    dataset = dataset.map(map_fn, remove_columns=dataset.column_names)
    true_answers = dataset["label"]

    if args.debug:
        dataset = dataset.select(range(30))
        true_answers = true_answers[:30]

    with open(args.models, "r") as f:
        model_list = json.load(f)
    models: Sequence[LLMTokenizerMapping] = [LLMTokenizerMapping(**model) for model in model_list]
    
    for item in models:
        language_model = item.language_model
        tokenizer = item.tokenizer
        logger.info(language_model)

        print(f"\nStudent model name: {language_model}\nEvaluated on {args.dataset} dataset\n")
        
        tokenizer = AutoTokenizer.from_pretrained(tokenizer, padding_side='left')
        tokenizer.pad_token = tokenizer.eos_token

        config = AutoConfig.from_pretrained(language_model)
        model = LanguageModel(config, language_model)

        if torch.cuda.device_count() > 1:
            model = torch.nn.DataParallel(model)
        
        MAX_NEW_TOKENS = calculate_avg_tokens(true_answers, tokenizer, args.avg_new_tokens)
        
        # get model context length
        model_max_length = get_max_length(config)
        
        # ensuring input length don't exceed `model_max_length-MAX_NEW_TOKENS`,
        # since `MAX_NEW_TOKENS` will be generated autoregressively
        max_length = model_max_length-MAX_NEW_TOKENS
        dataloader = torch.utils.data.DataLoader(dataset, batch_size=args.batch_size,
                        collate_fn=lambda x: collate_fn(batch=x, tokenizer=tokenizer, model_max_length=max_length))

        model.to(device)
        model.eval()
        
        predictions = []
        ground_truths = []
        count = 0
        with torch.no_grad():
            for batch, truths in tqdm(dataloader, total=len(dataloader)):
                input_ids = batch["input_ids"].to(device)
                attention_mask = batch["attention_mask"].to(device)
                outputs = model(input_ids=input_ids, attention_mask=attention_mask,
                                max_new_tokens=MAX_NEW_TOKENS, use_cache=True,
                                pad_token_id=tokenizer.pad_token_id)
                generated_text_batch = tokenizer.batch_decode(outputs, skip_special_tokens=True)
                
                # ignore text longer than model context length
                for generated_text, truth, inp_id in zip(generated_text_batch, truths, input_ids):
                    if PROMPT_KEYWORDS[args.task] not in tokenizer.decode(inp_id):
                        count += 1
                        continue
                    generated_preds = generated_text if config.is_encoder_decoder else generated_text.split(PROMPT_KEYWORDS[args.task])[1]
                    predictions.append(generated_preds)
                    ground_truths.append(truth)
                    
                # clean cache
                torch.cuda.empty_cache()

        if not predictions:   # if none of the inputs is shorter than model's context length
            logger.info(f"there are {count} empty predictions due to ignored long inputs")
            continue
            
        # Specify the path to the JSON file where you want to save the results
        model_name = language_model.split("/")[1].split("-")[0]
        if "uld_loss" in language_model:
            results_file_name = f'evalscript_{args.dataset}_{model_name}_uld_loss.json'
        elif "text_teacher" in language_model:
            results_file_name = f'evalscript_{args.dataset}_{model_name}_text_teacher.json'

        evaluate_lm(bertscore, rougescore, chrfscore, meteorscore, preds=predictions,
                    truth=ground_truths, logger=logger, output=args.output + f"/{args.task}/",
                    results_file_path=results_file_name)


if __name__ == '__main__':
    parser = argparse.ArgumentParser(description="Extract relations (triples) from a file of lecture notes")
    
    parser.add_argument("-t", "--task", type=str, default="summ",
                        help="The type of task that the student models are finetuned in: summ, qa")
    parser.add_argument("-d", "--dataset", type=str, default="hotpot_qa",
                        help="The path to the dataset used to evaluate the student models: hotpot_qa\
                            , truthful_qa, cnn_dm, or qmsum")
    parser.add_argument("-sub", "--subset", type=str, default="distractor",
                        help="The corresponding subset of the dataset: 'distractor' for hotpot_qa,\
                            'generation' for truthful_qa, '1.0.0' for cnn_dm, or 'default' for qmsum")
    parser.add_argument("-max", "--max_len", type=int, default=2000,
                        help="The maximum length of tokenized context") # for hotpot_qa majority of length values is < 1000
    parser.add_argument("-ms", "--models", type=str, default="./models_summ.json",
                        help="The path to the json file that specifies student model paths")
    parser.add_argument("-dm", "--data_mapper", type=str, default="./data_map.py",
                        help="The path to file that contains the data mappers")
    parser.add_argument("-a", "--avg_new_tokens", action='store_true', 
                        help="Whether to use average length of all answers in the dataset as the number of\
                                new tokens generate - if False then max length of all answers in the dataset")
    parser.add_argument("-db", "--debug", action='store_true', 
                        help="Whether or not to enable debug mode with toy dataset")
    parser.add_argument("-bs", "--batch_size", type=int, default=32,
                        help="Batch size for the data loader")
    parser.add_argument("-o", "--output", type=str, default="./eval_results",
                        help="The path to the folder to store evaluation output")

    args = parser.parse_args()
    
    main(args)
